<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Course Info on Class Homepage</title>
    <link>/docs/course-info/</link>
    <description>Recent content in Course Info on Class Homepage</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/docs/course-info/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Course Overview</title>
      <link>/docs/course-info/course-overview/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/course-info/course-overview/</guid>
      <description>Course Goals#Analyze NLP techniques and apply them to text data. The course is divided into three main categories: s
Preprocessing: Demonstrate how to clean and integrate text data
Processing: Apply NLP algorithms on your pre-processed data to perform different tasks
Post-processing: Evaluate your developed NLP models.
Solve problems with real datasets Apply practical know-how (useful for jobs, research) through significant hands-on programming assignments
Course Pre- and/or Co-Requisites#Review our &amp;ldquo;warnings&amp;rdquo; before taking this course.</description>
    </item>
    
    <item>
      <title>Instructor and TAs</title>
      <link>/docs/course-info/instructors-and-tas/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/course-info/instructors-and-tas/</guid>
      <description>Instructors and TAs#Instructor#Max Mahdi Roozbahanimahdir@gatech.eduhttps://mahdi-roozbahani.github.io/Wafa Louhichiwlouhichi3@gatech.eduhttps://www.linkedin.com/in/wafa-louhichi/Nimisha Roynroy9@gatech.eduhttps://nimisharoy9.wixsite.com/myportfolioHead TAs#Rusty Utomorutomo6@gatech.eduTAs#Rohit Dasrohdas@gatech.eduDaanish M Mohammeddmohammed7@gatech.eduRuijia Wangrwang415@gatech.eduDanrong Zhangdzhang373@gatech.edu</description>
    </item>
    
    <item>
      <title>Course Schedule</title>
      <link>/docs/course-info/course-schedule/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/docs/course-info/course-schedule/</guid>
      <description>Schedule#Important
All deadline and due dates in this course will be at AoE time zone. For all dates used in this course, their times are 23:59 Anywhere on Earth (11:59 pm AoE). For example, a due date of &amp;ldquo;January 8&amp;rdquo; is the same as &amp;ldquo;January 8, 23:59pm AoE&amp;rdquo;. Convert the times to your local times using a Time Zone Converter.
Scroll horizontally to see the full schedule table on mobile devicesWeekDatesTopicsHomeworkQuizzesReadings15/13 - 5/17Course introductionText data preprocessing: Normalization, lemmatization, stemming, stop words removal&amp;hellip;Text Representations:One hot encodingBoW (frequency counting)TF-IDFHW1 out 5/17Quiz 0 | Knowledge-based | out 5/13 - due 5/1725/20 - 5/24Classification IntroductionNaive BayesClassification Model Evaluation: accuracy, precision, recall, confusion matrixQuiz 1 | week 1 | out 5/17 - due 5/2435/27 - 5/31Memorial Day Official School holidayLogistic RegressionSVMPerceptronHW1 due 5/31HW2 out 5/31Quiz 2 | week 2 | out 5/24 - due 5/3146/03 - 6/07SVD (Dimensionality Reduction) + Co-occurrence embeddingsGloveQuiz 3 | week 3| out 5/31 - due 6/07GloVe: Global Vectors for Word Representation56/10 - 6/14Neural Network (fully connected)Word2vec: CBoW, Skip-GramHW2 due 6/14HW3 out 6/14Quiz 4 | week 4| out 6/07 - due 6/14NN PlaygroundInteractive NN initialization;The role of a hidden layer;Back propagation numerical example;More detailed introduction;Efficient Estimation of Word Representations in Vector Space66/17 - 6/21Juneteenth Official School HolidayCNNRNNQuiz 5 | week 5| out 6/14 - due 6/21CNN Live Demo;A guide to an efficient way to build CNN and optimize its hyper-parameters;Back Propagation in CNN;Transfer learning in CNN;76/24 - 6/28LSTM and GRULSTM + Attention (Focus on Attention mechanism)Quiz 6 | week 6| out 6/21 - due 6/28Understanding LSTM87/01 - 7/05Independence Day Official School holidayTransformer modelsExamples: BERT(Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer)HW3 due 7/05HW4 out 7/05Quiz 7 | week 7| out 6/28 - due 7/05Attention Is All You Need;BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding;97/08 - 7/12Sequence Labelling: POS TaggingSequence Labelling: NERQuiz 8 | week 8| out 7/05 - due 7/12107/15 - 7/19Unsupervised ModelsTopic Modeling (Latent Semantic Indexing, LDA (Latent Dirichlet Allocation)Quiz 9| week 9| out 7/12 - due 7/19117/22 - 7/25Focus on Homework 4HW4 due 07/25Quiz 10 | week 10 and 11 | out 7/19 - due 7/25</description>
    </item>
    
  </channel>
</rss>
